{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9986492991447449}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9998718500137329}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"text-classification\", \n",
    "                model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "result1 = pipe(\"The food in HKUST is terrible\")\n",
    "print(result1)\n",
    "\n",
    "result2 = pipe(\"This course is a awesome\")\n",
    "print(result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/biyuan/anaconda3/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1154: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
      "  warnings.warn(\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Hugging Face est un platform d'open-source pour l'apprentissage de machine.\"}]\n"
     ]
    }
   ],
   "source": [
    "text = \"translate English to French: Hugging Face is a community-based open-source platform for machine learning.\"\n",
    "translator = pipeline(task=\"translation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "text_in_chinese = translator(text)\n",
    "print(text_in_chinese)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 200, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "/Users/biyuan/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:1427: UserWarning: Unfeasible length constraints: `min_length` (30) is larger than the maximum possible length (10). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length. Note that `max_length` is set to 10, its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Summarization creates a shorter version'}]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(task=\"summarization\", model=\"google/flan-t5-small\", max_new_tokens=9)\n",
    "\n",
    "text =  \"Summarization creates a shorter version of a text from a longer one while trying to preserve most of the meaning of the original document. \"\n",
    "result = summarizer(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Which university is best in Europe?\\n\\nThe University of Oxford is the most popular university in'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(task=\"text-generation\", model=\"facebook/opt-350m\")\n",
    "prompt_text = \"Which university is best in Europe?\"\n",
    "result = generator(prompt_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<test_model> initialization start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<test_model> initialization done\n",
      "start_ids: length (1) ids: tensor([[    2, 32251,  2737,    16,   275,    11,  1005,   116]])\n",
      "[INFO] raw output: odict_keys(['sequences', 'scores', 'past_key_values']) 3, torch.Size([1, 24]),  (torch.Size([1, 50272]),torch.Size([1, 50272])) 24\n",
      "[INFO] raw token: tensor([50118, 50118,   133,   589,     9,  9238,    16,     5,   275,  2737,\n",
      "           11,  1005,     4,    85,    16,    10])\n",
      "[INFO] \n",
      "[Context]\n",
      "Which university is best in Europe?\n",
      "\n",
      "[Output]\n",
      "\n",
      "\n",
      "The University of Oxford is the best university in Europe. It is a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from transformers.models.opt.modeling_opt import *\n",
    "\n",
    "\n",
    "def generate(task_info, device, model, tokenizer):\n",
    "    contexts = task_info[\"prompt_seqs\"]\n",
    "    inputs = tokenizer(contexts, return_tensors=\"pt\").to(device)\n",
    "    print(f\"start_ids: length ({inputs.input_ids.shape[0]}) ids: {inputs.input_ids}\")\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, do_sample=True, top_p=task_info['top_p'],\n",
    "        temperature=1.0, top_k=1,\n",
    "        max_new_tokens=task_info[\"output_len\"],\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,  # return logit score\n",
    "        output_hidden_states=False,  # return embeddings\n",
    "    )\n",
    "    print(f\"[INFO] raw output: {outputs.keys()} {len(outputs)}, {outputs[0].shape},  ({outputs[1][0].shape},{outputs[1][1].shape}) {len(outputs[2])}\")\n",
    "    token = outputs.sequences[0, input_length:]  # exclude context input from the output\n",
    "    print(f\"[INFO] raw token: {token}\")\n",
    "    output = tokenizer.decode(token)\n",
    "    print(f\"[INFO] \\n[Context]\\n{contexts}\\n\\n[Output]\\n{output}\\n\")\n",
    "\n",
    "\n",
    "def test_model(args):\n",
    "    print(f\"<test_model> initialization start\")\n",
    "    device = torch.device(args.get('device', 'cpu'))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args['hf_model_name'])\n",
    "    model = AutoModelForCausalLM.from_pretrained(args['hf_model_name'])\n",
    "    model = model.to(device)\n",
    "    torch.manual_seed(0)\n",
    "    task_info = {\n",
    "        \"seed\": 0,\n",
    "        \"prompt_seqs\": args.get(\"prompt\"),\n",
    "        \"output_len\": 16,\n",
    "        \"beam_width\": 1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 1,\n",
    "        \"beam_search_diversity_rate\": 0,\n",
    "        \"len_penalty\": 0,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "        \"stop\": args.get(\"stop\", []),\n",
    "        \"logprobs\": 5,\n",
    "    }\n",
    "    print(f\"<test_model> initialization done\")\n",
    "    generate(task_info, device, model, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "test_model(args={\n",
    "    \"hf_model_name\": 'facebook/opt-350m',\n",
    "    \"interactive\": True,\n",
    "    \"device\": \"cpu\",\n",
    "    \"dtype\": torch.float32,\n",
    "    \"prompt\": \"Which university is best in Europe?\",\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
